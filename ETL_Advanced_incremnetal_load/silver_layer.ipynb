{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250111bc-9e78-48bd-923b-a0bf2f5bb346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./raw_data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b629bc-a428-4ea3-b1da-c7fbd15c514c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"started for creating the silverlayer table\")\n",
    "    create_silvertable = (f'''\n",
    "        CREATE TABLE IF NOT EXISTS etl_incremental.silver.patient_silver (\n",
    "        patient_id INT,\n",
    "        name STRING,\n",
    "        age INT,\n",
    "        gender STRING,\n",
    "        city STRING,\n",
    "        disease STRING,\n",
    "        bill_amount INT,\n",
    "        start_date DATE,\n",
    "        end_date DATE,\n",
    "        is_current BOOLEAN\n",
    "        )\n",
    "        USING DELTA\n",
    "        PARTITIONED BY (city)'''\n",
    "\n",
    "    )\n",
    "    spark.sql(create_silvertable)\n",
    "    logger.info(\"created the silverlayer table\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating silverlayer table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312e22d3-43fe-47a9-a943-a3360c408d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    logger.info(\"Added housekeeping columns to raw data\")\n",
    "    silver_init = raw_data \\\n",
    "    .withColumn(\"start_date\", current_date()) \\\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"date\")) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "    silver_init.write.format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(\"etl_incremental.silver.patient_silver\")\n",
    "    logger.info(\"Added housekeeping columns to raw data\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Error while adding housekeeping columns to raw data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65da424d-c884-422f-b5f0-98dae58c3592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# inc_df=spark.read.format('csv').option('header', 'true').load(incremental_file)\n",
    "# \n",
    "inc_df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(incremental_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dcde5fc-005a-45f2-9816-568c25e2fc2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "# Deduplicate source DataFrame by patient_id\n",
    "inc_df_dedup = inc_df.dropDuplicates([\"patient_id\"])\n",
    "\n",
    "silver_tbl = DeltaTable.forName(spark, \"etl_incremental.silver.patient_silver\")\n",
    "\n",
    "# Step 1: Close old records\n",
    "silver_tbl.alias(\"t\").merge(\n",
    "    inc_df_dedup.alias(\"s\"),\n",
    "    \"t.patient_id = s.patient_id AND t.is_current = true\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"t.city <> s.city OR t.bill_amount <> s.bill_amount\",\n",
    "    set={\n",
    "        \"end_date\": \"current_date()\",\n",
    "        \"is_current\": \"false\"\n",
    "    }\n",
    ").execute()\n",
    "\n",
    "# Step 2: Insert new version\n",
    "new_records = inc_df_dedup \\\n",
    "    .withColumn(\"start_date\", current_date()) \\\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"date\")) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "new_records.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"etl_incremental.silver.patient_silver\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7768535042746902,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_layer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
